{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pipeline.resnet_csra import ResNet_CSRA\n",
    "from pipeline.vit_csra import VIT_B16_224_CSRA, VIT_L16_224_CSRA, VIT_CSRA\n",
    "from pipeline.dataset import DataSet\n",
    "from utils.evaluation.eval import evaluation\n",
    "from utils.evaluation.eval import WarmUpLR\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Args():\n",
    "    parser = argparse.ArgumentParser(description=\"settings\")\n",
    "    # model default resnet101\n",
    "    parser.add_argument(\"--model\", default=\"resnet101\", type=str)\n",
    "    parser.add_argument(\"--num_heads\", default=1, type=int)\n",
    "    parser.add_argument(\"--lam\",default=0.1, type=float)\n",
    "    parser.add_argument(\"--cutmix\", default=None, type=str) # the path to load cutmix-pretrained backbone\n",
    "    parser.add_argument(\"--load_from\", default=\"models_local/resnet101_voc07_head1_lam0.1_94.7.pth\", type=str)\n",
    "    # dataset\n",
    "    parser.add_argument(\"--dataset\", default=\"voc07\", type=str)\n",
    "    parser.add_argument(\"--num_cls\", default=20, type=int)\n",
    "    parser.add_argument(\"--test_aug\", default=[], type=list)\n",
    "    parser.add_argument(\"--img_size\", default=448, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=16, type=int)\n",
    "\n",
    "    args = parser.parse_args(\"\") # \"\" added because to work with jupyter notebooks\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def val(args, model, test_loader, test_file):\n",
    "    model.eval()\n",
    "    print(\"Test on Pretrained Models\")\n",
    "    result_list = []\n",
    "\n",
    "    # calculate logit\n",
    "    for index, data in enumerate(tqdm(test_loader)):\n",
    "        img = data['img'].cuda()\n",
    "        target = data['target'].cuda()\n",
    "        img_path = data['img_path']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "\n",
    "        result = nn.Sigmoid()(logit).cpu().detach().numpy().tolist()\n",
    "        for k in range(len(img_path)):\n",
    "            result_list.append(\n",
    "                {\n",
    "                    \"file_name\": img_path[k].split(\"/\")[-1].split(\".\")[0],\n",
    "                    \"scores\": result[k]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    # cal_mAP OP OR\n",
    "    evaluation(result=result_list, types=args.dataset, ann_path=test_file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC \n",
    "\n",
    "##### Base\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py --exp_name base_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "##### MSL\n",
    "CUDA_VISIBLE_DEVICES=0 python train_masksup.py --exp_name masksup_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "##### Test\n",
    "CUDA_VISIBLE_DEVICES=0 python val.py --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20  --load_from checkpoint/voc_experiments/rescm_paper_voc/epoch_200.pth --cutmix data/resnet101_cutmix_pretrained.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_heads = 1\n",
    "args.lam = 0.1\n",
    "args.dataset = \"voc07\"\n",
    "args.num_cls = 20\n",
    "args.load_from = \"checkpoint/voc_experiments/rescm_paper_voc_base/epoch_48.pth\"\n",
    "args.cutmix = \"data/resnet101_cutmix_pretrained.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "if args.model == \"resnet101\": \n",
    "    model = ResNet_CSRA(num_heads=args.num_heads, lam=args.lam, num_classes=args.num_cls, cutmix=args.cutmix)\n",
    "if args.model == \"vit_B16_224\":\n",
    "    model = VIT_B16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "if args.model == \"vit_L16_224\":\n",
    "    model = VIT_L16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading weights from {}\".format(args.load_from))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"lets use {} GPUs.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model.module.load_state_dict(torch.load(args.load_from))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.load_from))\n",
    "\n",
    "# data\n",
    "if args.dataset == \"voc07\":\n",
    "    test_file = ['data/voc07/test_voc07.json']\n",
    "if args.dataset == \"coco\":\n",
    "    test_file = ['data/coco/val_coco2014.json']\n",
    "if args.dataset == \"wider\":\n",
    "    test_file = ['data/wider/test_wider.json']\n",
    "\n",
    "\n",
    "test_dataset = DataSet(test_file, args.test_aug, args.img_size, args.dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val(args, model, test_loader, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "result_list = []\n",
    "\n",
    "# calculate logit\n",
    "for index, data in enumerate(tqdm(test_loader)):\n",
    "    img = data['img'].cuda()\n",
    "    target = data['target'].cuda()\n",
    "    img_path = data['img_path']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = model(img)\n",
    "\n",
    "    result = nn.Sigmoid()(logit).cpu().detach().numpy().tolist()\n",
    "    for k in range(len(img_path)):\n",
    "        result_list.append(\n",
    "            {\n",
    "                \"file_name\": img_path[k].split(\"/\")[-1].split(\".\")[0],\n",
    "                \"scores\": result[k]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def to_img(ten):\n",
    "#     ten =(ten[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "#     ten=(ten*255).astype(np.uint8)\n",
    "#     return ten\n",
    "\n",
    "def unnormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def to_img_(ten):\n",
    "    curr_img = ten.detach().to(torch.device('cpu'))\n",
    "    curr_img = unnormalize(curr_img,\n",
    "                           torch.tensor([0, 0, 0]), # mean and std\n",
    "                           torch.tensor([1, 1, 1])) \n",
    "    curr_img = curr_img.permute((1, 2, 0))\n",
    "    return curr_img\n",
    "\n",
    "\n",
    "# a = to_img_(x[7])\n",
    "# print(a.shape)\n",
    "# plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = to_img_(img[7])\n",
    "# print(a.shape)\n",
    "# plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_images = 5\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=n_images, \n",
    "#                          sharex=True, sharey=True, figsize=(20, 2.5))\n",
    "\n",
    "# for i in range(5):\n",
    "#     axes[i].imshow(to_img_(img[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('maskrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35d972689a4ebd6112cf5bf9eea2c3bb189b2972b77b117bc02bba8b4bbbd65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

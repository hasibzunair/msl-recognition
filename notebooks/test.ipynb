{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions from a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pipeline.resnet_csra import ResNet_CSRA\n",
    "from pipeline.vit_csra import VIT_B16_224_CSRA, VIT_L16_224_CSRA, VIT_CSRA\n",
    "from pipeline.dataset import DataSet\n",
    "from utils.evaluation.eval import evaluation\n",
    "from utils.evaluation.eval import WarmUpLR\n",
    "from utils.evaluation.eval import class_dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Args():\n",
    "    parser = argparse.ArgumentParser(description=\"settings\")\n",
    "    # model default resnet101\n",
    "    parser.add_argument(\"--model\", default=\"resnet101\", type=str)\n",
    "    parser.add_argument(\"--num_heads\", default=1, type=int)\n",
    "    parser.add_argument(\"--lam\",default=0.1, type=float)\n",
    "    parser.add_argument(\"--cutmix\", default=None, type=str) # the path to load cutmix-pretrained backbone\n",
    "    parser.add_argument(\"--load_from\", default=\"models_local/resnet101_voc07_head1_lam0.1_94.7.pth\", type=str)\n",
    "    # dataset\n",
    "    parser.add_argument(\"--dataset\", default=\"voc07\", type=str)\n",
    "    parser.add_argument(\"--num_cls\", default=20, type=int)\n",
    "    parser.add_argument(\"--test_aug\", default=[], type=list)\n",
    "    parser.add_argument(\"--img_size\", default=448, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=16, type=int)\n",
    "\n",
    "    args = parser.parse_args(\"\") # \"\" added because to work with jupyter notebooks\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def val(args, model, test_loader, test_file):\n",
    "    model.eval()\n",
    "    print(\"Test on Pretrained Models\")\n",
    "    result_list = []\n",
    "\n",
    "    # calculate logit\n",
    "    for index, data in enumerate(tqdm(test_loader)):\n",
    "        img = data['img'].cuda()\n",
    "        target = data['target'].cuda()\n",
    "        img_path = data['img_path']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "\n",
    "        result = nn.Sigmoid()(logit).cpu().detach().numpy().tolist()\n",
    "        for k in range(len(img_path)):\n",
    "            result_list.append(\n",
    "                {\n",
    "                    \"file_name\": img_path[k].split(\"/\")[-1].split(\".\")[0],\n",
    "                    \"scores\": result[k]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    # cal_mAP OP OR\n",
    "    evaluation(result=result_list, types=args.dataset, ann_path=test_file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VOC \n",
    "\n",
    "Base\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py --exp_name base_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "MSL\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train_masksup.py --exp_name masksup_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "Test\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python val.py --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20  --load_from checkpoint/voc_experiments/rescm_paper_voc/epoch_200.pth --cutmix data/resnet101_cutmix_pretrained.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC\n",
    "# args.num_heads = 1\n",
    "# args.lam = 0.1\n",
    "# args.dataset = \"voc07\"\n",
    "# args.num_cls = 20\n",
    "# args.load_from = \"checkpoint/voc_experiments/masksup01_0.3,0.2,0.5_rescm_voc/epoch_54.pth\" # here\n",
    "\n",
    "# rescm_paper_voc_base/epoch_48.pth (Baseline)\n",
    "# masksup01_0.3,0.2,0.5_rescm_voc/epoch_54.pth (MSL)\n",
    "\n",
    "# COCO\n",
    "args.num_heads = 6\n",
    "args.lam = 0.4\n",
    "args.dataset = \"coco\" #voc07, 20, coco, 80\n",
    "args.num_cls = 80\n",
    "\n",
    "# rescm_paper_coco_base/epoch_43.pth\n",
    "# masksup01_0.3,0.2,0.5_rescm_coco/epoch_58.pth\n",
    "args.load_from = \"checkpoint/coco_experiments/masksup01_0.3,0.2,0.5_rescm_coco/epoch_58.pth\"\n",
    "exp_name = \"masksup01_0.3,0.2,0.5_rescm_coco\"\n",
    "\n",
    "#args.cutmix = \"data/resnet101_cutmix_pretrained.pth\"\n",
    "args.batch_size = 1\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "if args.model == \"resnet101\": \n",
    "    model = ResNet_CSRA(num_heads=args.num_heads, lam=args.lam, num_classes=args.num_cls, cutmix=args.cutmix)\n",
    "if args.model == \"vit_B16_224\":\n",
    "    model = VIT_B16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "if args.model == \"vit_L16_224\":\n",
    "    model = VIT_L16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "print(\"Loading weights from {}\".format(args.load_from))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"lets use {} GPUs.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model.module.load_state_dict(torch.load(args.load_from))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.load_from))\n",
    "\n",
    "# data\n",
    "if args.dataset == \"voc07\":\n",
    "    test_file = ['data/voc07/test_voc07.json']\n",
    "if args.dataset == \"coco\":\n",
    "    test_file = ['data/coco/val_coco2014.json']\n",
    "if args.dataset == \"wider\":\n",
    "    test_file = ['data/wider/test_wider.json']\n",
    "\n",
    "\n",
    "test_dataset = DataSet(test_file, args.test_aug, args.img_size, args.dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "val(args, model, test_loader, test_file) # this should match value in spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# Helpers\n",
    "\n",
    "# Font Size\n",
    "import matplotlib\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 30}\n",
    "\n",
    "import torch \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unnormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def read_resize_image(img, new_width=320, new_height=320):\n",
    "    img = ImageOps.fit(img, (new_width, new_height), Image.BICUBIC)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "def to_img_(ten):\n",
    "    curr_img = ten.detach().to(torch.device('cpu'))\n",
    "    curr_img = unnormalize(curr_img,\n",
    "                           torch.tensor([0, 0, 0]), # mean and std\n",
    "                           torch.tensor([1, 1, 1])) \n",
    "    curr_img = curr_img.permute((1, 2, 0))\n",
    "    curr_img = (curr_img.numpy()*255).astype(np.uint8)\n",
    "    curr_img = Image.fromarray(np.uint8(curr_img)).convert('RGB')\n",
    "    curr_img = read_resize_image(curr_img)\n",
    "    return curr_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"./checkpoint/analysis/prediction_visualizations/\" + exp_name\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "\n",
    "for index, data in enumerate(tqdm(test_loader)):\n",
    "\n",
    "    # Load data\n",
    "    img = data[\"img\"].cuda()\n",
    "    img_path = data[\"img_path\"]\n",
    "    target = data[\"target\"].cuda()\n",
    "\n",
    "    # Get class names from labels\n",
    "    post = torch.where(target[0]  > 0.5)[0].cpu().numpy()\n",
    "    gts = []\n",
    "    for k in post:\n",
    "        gts.append(class_dict[args.dataset][k])\n",
    "\n",
    "    if len(gts) == 3:\n",
    "        # Predict\n",
    "        logit = model(img).squeeze(0)\n",
    "        logit = nn.Sigmoid()(logit)\n",
    "\n",
    "        # Get class names from predictions\n",
    "        pos = torch.where(logit > 0.5)[0].cpu().numpy()\n",
    "        predictions = []\n",
    "        for k in pos:\n",
    "            predictions.append(class_dict[args.dataset][k])\n",
    "\n",
    "        # Do this for adding preds as captions with images\n",
    "        # #print(predictions, gts)\n",
    "        # caption = str(predictions)[1:-1]\n",
    "        # caption_gt = str(gts)[1:-1]\n",
    "        # fig = plt.figure()\n",
    "        # im = to_img_(img[0])\n",
    "        # plt.imshow(im)\n",
    "        # plt.axis(\"off\")\n",
    "        # fig.text(.5, .07, caption, ha='center', size=11)\n",
    "        # fig.text(.5, .008, \"GT: \" + caption_gt, ha='center', size=9)\n",
    "        # plt.savefig(LOG_PATH + \"/\" + f\"{index}.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=300)\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "        # Do this for adding preds as captions inside of the images\n",
    "        im = to_img_(img[0])\n",
    "        draw = im\n",
    "\n",
    "        gt = gts \n",
    "        yhat = predictions\n",
    "\n",
    "        if gt[0] in yhat:\n",
    "            image = cv2.putText(draw, f\"{gt[0]}\", (30, 220), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            image = cv2.putText(draw, f\"{gt[0]}\", (30, 220), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if gt[1] in yhat:\n",
    "            image = cv2.putText(draw, f\"{gt[1]}\", (30, 260), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            image = cv2.putText(draw, f\"{gt[1]}\", (30, 260), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if gt[2] in yhat:\n",
    "            image = cv2.putText(draw, f\"{gt[2]}\", (30, 300), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            image = cv2.putText(draw, f\"{gt[2]}\", (30, 300), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        bgr=cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(LOG_PATH + \"/\" + f\"{index}.png\",bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vis outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from natsort import natsorted\n",
    "\n",
    "macos = False\n",
    "if macos == True:\n",
    "    rc('font',**{'family':'sans-serif','sans-serif':['Computer Modern Roman']})\n",
    "    rc('text', usetex=True)\n",
    "\n",
    "# Font Size\n",
    "import matplotlib\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 30}\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "\n",
    "def visualize(idx, idx_flag=True, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\" \n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(60, 40))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        #if idx==0:\n",
    "        plt.title(' '.join(name.split('_')).lower(), fontsize=60)\n",
    "        if idx_flag:\n",
    "            if i ==0:\n",
    "                w,h = (1,25)\n",
    "                fs = 1.0\n",
    "                color = (0,0,0)\n",
    "                #color = (255,255,255)\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX #FONT_HERSHEY_DUPLEX  #press tab for different operations\n",
    "                cv2.putText(image, str(idx), (w,h), font, fs, color, 1, cv2.LINE_AA)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        #plt.tight_layout()\n",
    "    plt.savefig(\"./checkpoint/analysis/prediction_visualizations/voc/compare/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    #plt.close()\n",
    "\n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    f = dir.split('/')[-1].split('_')[-1]\n",
    "    #print (dir, f)\n",
    "    dirs= os.listdir(dir)\n",
    "    for img in dirs:\n",
    "\n",
    "        path = os.path.join(dir, img)\n",
    "        #print(path)\n",
    "        images.append(path)\n",
    "    return images\n",
    "\n",
    "def read_image(path):\n",
    "    image = cv2.imread(path, -1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def read_image_(path):\n",
    "    image = cv2.imread(path, -1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (192, 256))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"./checkpoint/analysis/prediction_visualizations/voc/rescm_paper_voc_base\" \n",
    "msl = \"./checkpoint/analysis/prediction_visualizations/voc/masksup01_0.3,0.2,0.5_rescm_voc\" \n",
    "\n",
    "#base = \"./checkpoint/analysis/masked_prediction_visualizations/coco/rescm_paper_coco_base\" \n",
    "#msl = \"./checkpoint/analysis/masked_prediction_visualizations/coco/masksup01_0.3,0.2,0.5_rescm_coco\" \n",
    "\n",
    "# File paths to tryon images\n",
    "base = natsorted(make_dataset(base))\n",
    "msl = natsorted(make_dataset(msl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for num in range(len(msl)):\n",
    "    visualize(i, idx_flag=False, baseline=read_image(base[num]), masksup=read_image(msl[num])\n",
    "             )\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('maskrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35d972689a4ebd6112cf5bf9eea2c3bb189b2972b77b117bc02bba8b4bbbd65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

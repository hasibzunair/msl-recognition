{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions from a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hasib/masksup-recognition\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pipeline.resnet_csra import ResNet_CSRA\n",
    "from pipeline.vit_csra import VIT_B16_224_CSRA, VIT_L16_224_CSRA, VIT_CSRA\n",
    "from pipeline.dataset import DataSet\n",
    "from utils.evaluation.eval import evaluation\n",
    "from utils.evaluation.eval import WarmUpLR\n",
    "from utils.evaluation.eval import class_dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Args():\n",
    "    parser = argparse.ArgumentParser(description=\"settings\")\n",
    "    # model default resnet101\n",
    "    parser.add_argument(\"--model\", default=\"resnet101\", type=str)\n",
    "    parser.add_argument(\"--num_heads\", default=1, type=int)\n",
    "    parser.add_argument(\"--lam\",default=0.1, type=float)\n",
    "    parser.add_argument(\"--cutmix\", default=None, type=str) # the path to load cutmix-pretrained backbone\n",
    "    parser.add_argument(\"--load_from\", default=\"models_local/resnet101_voc07_head1_lam0.1_94.7.pth\", type=str)\n",
    "    # dataset\n",
    "    parser.add_argument(\"--dataset\", default=\"voc07\", type=str)\n",
    "    parser.add_argument(\"--num_cls\", default=20, type=int)\n",
    "    parser.add_argument(\"--test_aug\", default=[], type=list)\n",
    "    parser.add_argument(\"--img_size\", default=448, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=16, type=int)\n",
    "\n",
    "    args = parser.parse_args(\"\") # \"\" added because to work with jupyter notebooks\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def val(args, model, test_loader, test_file):\n",
    "    model.eval()\n",
    "    print(\"Test on Pretrained Models\")\n",
    "    result_list = []\n",
    "\n",
    "    # calculate logit\n",
    "    for index, data in enumerate(tqdm(test_loader)):\n",
    "        img = data['img'].cuda()\n",
    "        target = data['target'].cuda()\n",
    "        img_path = data['img_path']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "\n",
    "        result = nn.Sigmoid()(logit).cpu().detach().numpy().tolist()\n",
    "        for k in range(len(img_path)):\n",
    "            result_list.append(\n",
    "                {\n",
    "                    \"file_name\": img_path[k].split(\"/\")[-1].split(\".\")[0],\n",
    "                    \"scores\": result[k]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    # cal_mAP OP OR\n",
    "    evaluation(result=result_list, types=args.dataset, ann_path=test_file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VOC \n",
    "\n",
    "Base\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py --exp_name base_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "MSL\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train_masksup.py --exp_name masksup_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "Test\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python val.py --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20  --load_from checkpoint/voc_experiments/rescm_paper_voc/epoch_200.pth --cutmix data/resnet101_cutmix_pretrained.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=16, cutmix=None, dataset='voc07', img_size=448, lam=0.1, load_from='models_local/resnet101_voc07_head1_lam0.1_94.7.pth', model='resnet101', num_cls=20, num_heads=1, test_aug=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Args()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=1, cutmix='data/resnet101_cutmix_pretrained.pth', dataset='voc07', img_size=448, lam=0.1, load_from='checkpoint/voc_experiments/rescm_paper_voc_base/epoch_48.pth', model='resnet101', num_cls=20, num_heads=1, test_aug=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_heads = 1\n",
    "args.lam = 0.1\n",
    "args.dataset = \"voc07\"\n",
    "args.num_cls = 20\n",
    "args.load_from = \"checkpoint/voc_experiments/rescm_paper_voc_base/epoch_48.pth\"\n",
    "exp_name = \"rescm_paper_voc_base\"\n",
    "args.cutmix = \"data/resnet101_cutmix_pretrained.pth\"\n",
    "args.batch_size = 1\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone params inited by CutMix pretrained model\n",
      "Loading weights from checkpoint/voc_experiments/rescm_paper_voc_base/epoch_48.pth\n",
      "Compose(\n",
      "    Resize(size=(448, 448), interpolation=bilinear)\n",
      ")\n",
      "Test on Pretrained Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4952/4952 [01:51<00:00, 44.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 465.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.937091\n",
      "CP: 0.892378, CR: 0.875727, CF1 :0.883974\n",
      "OP: 0.921123, OR: 0.889206, OF1 0.904883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "if args.model == \"resnet101\": \n",
    "    model = ResNet_CSRA(num_heads=args.num_heads, lam=args.lam, num_classes=args.num_cls, cutmix=args.cutmix)\n",
    "if args.model == \"vit_B16_224\":\n",
    "    model = VIT_B16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "if args.model == \"vit_L16_224\":\n",
    "    model = VIT_L16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "print(\"Loading weights from {}\".format(args.load_from))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"lets use {} GPUs.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model.module.load_state_dict(torch.load(args.load_from))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.load_from))\n",
    "\n",
    "# data\n",
    "if args.dataset == \"voc07\":\n",
    "    test_file = ['data/voc07/test_voc07.json']\n",
    "if args.dataset == \"coco\":\n",
    "    test_file = ['data/coco/val_coco2014.json']\n",
    "if args.dataset == \"wider\":\n",
    "    test_file = ['data/wider/test_wider.json']\n",
    "\n",
    "\n",
    "test_dataset = DataSet(test_file, args.test_aug, args.img_size, args.dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "val(args, model, test_loader, test_file) # this should match value in spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "# Font Size\n",
    "import matplotlib\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 30}\n",
    "\n",
    "import torch \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unnormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def to_img_(ten):\n",
    "    curr_img = ten.detach().to(torch.device('cpu'))\n",
    "    curr_img = unnormalize(curr_img,\n",
    "                           torch.tensor([0, 0, 0]), # mean and std\n",
    "                           torch.tensor([1, 1, 1])) \n",
    "    curr_img = curr_img.permute((1, 2, 0))\n",
    "    return curr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint  demo.py\t     notebooks\t__pycache__\t  train.py\n",
      "data\t    environment.yml  notes.md\tREADME.md\t  utils\n",
      "datasets    helpers.py\t     pipeline\ttrain_masksup.py  val.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"./checkpoint/analysis/prediction_visualizations/\" + exp_name\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'person'] ['dog', 'person']\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "\n",
    "for index, data in enumerate(tqdm(test_loader)):\n",
    "\n",
    "    # Load data\n",
    "    img = data[\"img\"].cuda()\n",
    "    img_path = data[\"img_path\"]\n",
    "    target = data[\"target\"].cuda()\n",
    "\n",
    "    # Predict\n",
    "    logit = model(img).squeeze(0)\n",
    "    logit = nn.Sigmoid()(logit)\n",
    "\n",
    "    # Get class names from predictions\n",
    "    pos = torch.where(logit > 0.5)[0].cpu().numpy()\n",
    "    predictions = []\n",
    "    for k in pos:\n",
    "        predictions.append(class_dict[args.dataset][k])\n",
    "\n",
    "    # Get class names from labels\n",
    "    post = torch.where(target[0]  > 0.5)[0].cpu().numpy()\n",
    "    gts = []\n",
    "    for k in pos:\n",
    "        gts.append(class_dict[args.dataset][k])\n",
    "\n",
    "    #print(predictions, gts)\n",
    "    caption = str(predictions)[1:-1]\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(to_img_(img[0]))\n",
    "    plt.axis(\"off\")\n",
    "    fig.text(.5, .05, caption, ha='center', size=20)\n",
    "    plt.savefig(LOG_PATH + f\"{index}.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'dog', 'person'\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "# make folder and save images in them (folder should have similar name as saved model)\n",
    "# compare gt and pred, if correct label in green, if wrong, label in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([448, 448, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = to_img_(img[0])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "result_list = []\n",
    "\n",
    "# calculate logit\n",
    "for index, data in enumerate(tqdm(test_loader)):\n",
    "    img = data['img'].cuda()\n",
    "    target = data['target'].cuda()\n",
    "    img_path = data['img_path']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = model(img)\n",
    "\n",
    "    \n",
    "    # result = nn.Sigmoid()(logit).cpu().detach().numpy().tolist()\n",
    "    # for k in range(len(img_path)):\n",
    "    #     result_list.append(\n",
    "    #         {\n",
    "    #             \"file_name\": img_path[k].split(\"/\")[-1].split(\".\")[0],\n",
    "    #             \"scores\": result[k]\n",
    "    #         }\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('maskrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35d972689a4ebd6112cf5bf9eea2c3bb189b2972b77b117bc02bba8b4bbbd65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions from a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "#sys.path.insert(0,\"..\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pipeline.resnet_csra import ResNet_CSRA\n",
    "from pipeline.vit_csra import VIT_B16_224_CSRA, VIT_L16_224_CSRA, VIT_CSRA\n",
    "from pipeline.dataset import DataSet\n",
    "from utils.evaluation.eval import evaluation\n",
    "from utils.evaluation.eval import WarmUpLR\n",
    "from utils.evaluation.eval import class_dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Args():\n",
    "    parser = argparse.ArgumentParser(description=\"settings\")\n",
    "    # model default resnet101\n",
    "    parser.add_argument(\"--model\", default=\"resnet101\", type=str)\n",
    "    parser.add_argument(\"--num_heads\", default=1, type=int)\n",
    "    parser.add_argument(\"--lam\",default=0.1, type=float)\n",
    "    parser.add_argument(\"--cutmix\", default=None, type=str) # the path to load cutmix-pretrained backbone\n",
    "    parser.add_argument(\"--load_from\", default=\"models_local/resnet101_voc07_head1_lam0.1_94.7.pth\", type=str)\n",
    "    # dataset\n",
    "    parser.add_argument(\"--dataset\", default=\"voc07\", type=str)\n",
    "    parser.add_argument(\"--num_cls\", default=20, type=int)\n",
    "    parser.add_argument(\"--test_aug\", default=[], type=list)\n",
    "    parser.add_argument(\"--img_size\", default=448, type=int)\n",
    "    parser.add_argument(\"--batch_size\", default=16, type=int)\n",
    "\n",
    "    args = parser.parse_args(\"\") # \"\" added because to work with jupyter notebooks\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def val(args, model, test_loader, test_file):\n",
    "    model.eval()\n",
    "    print(\"Test on Pretrained Models\")\n",
    "    result_list = []\n",
    "\n",
    "    # calculate logit\n",
    "    for index, data in enumerate(tqdm(test_loader)):\n",
    "        img = data['img'].cuda()\n",
    "        target = data['target'].cuda()\n",
    "        img_path = data['img_path']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = model(img)\n",
    "\n",
    "        result = nn.Sigmoid()(logit).cpu().detach().numpy().tolist()\n",
    "        for k in range(len(img_path)):\n",
    "            result_list.append(\n",
    "                {\n",
    "                    \"file_name\": img_path[k].split(\"/\")[-1].split(\".\")[0],\n",
    "                    \"scores\": result[k]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    # cal_mAP OP OR\n",
    "    evaluation(result=result_list, types=args.dataset, ann_path=test_file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VOC \n",
    "\n",
    "Base\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train.py --exp_name base_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "MSL\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python train_masksup.py --exp_name masksup_rescm_voc --batch_size 6 --total_epoch 60 --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20 --cutmix data/resnet101_cutmix_pretrained.pth\n",
    "\n",
    "Test\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python val.py --num_heads 1 --lam 0.1 --dataset voc07 --num_cls 20  --load_from checkpoint/voc_experiments/rescm_paper_voc/epoch_200.pth --cutmix data/resnet101_cutmix_pretrained.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_heads = 1\n",
    "args.lam = 0.1\n",
    "args.dataset = \"voc07\"\n",
    "args.num_cls = 20\n",
    "\n",
    "\n",
    "args.load_from = \"checkpoint/voc_experiments/masksup01_0.3,0.2,0.5_rescm_voc/epoch_54.pth\" # here\n",
    "# rescm_paper_voc_base/epoch_48.pth\n",
    "# masksup01_0.3,0.2,0.5_rescm_voc/epoch_54.pth\n",
    "exp_name = \"masksup01_0.3,0.2,0.5_rescm_voc\" # here too\n",
    "\n",
    "\n",
    "#args.cutmix = \"data/resnet101_cutmix_pretrained.pth\"\n",
    "args.batch_size = 1\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "if args.model == \"resnet101\": \n",
    "    model = ResNet_CSRA(num_heads=args.num_heads, lam=args.lam, num_classes=args.num_cls, cutmix=args.cutmix)\n",
    "if args.model == \"vit_B16_224\":\n",
    "    model = VIT_B16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "if args.model == \"vit_L16_224\":\n",
    "    model = VIT_L16_224_CSRA(cls_num_heads=args.num_heads, lam=args.lam, cls_num_cls=args.num_cls)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "print(\"Loading weights from {}\".format(args.load_from))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"lets use {} GPUs.\".format(torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model.module.load_state_dict(torch.load(args.load_from))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(args.load_from))\n",
    "\n",
    "# data\n",
    "if args.dataset == \"voc07\":\n",
    "    test_file = ['data/voc07/test_voc07.json']\n",
    "if args.dataset == \"coco\":\n",
    "    test_file = ['data/coco/val_coco2014.json']\n",
    "if args.dataset == \"wider\":\n",
    "    test_file = ['data/wider/test_wider.json']\n",
    "\n",
    "\n",
    "test_dataset = DataSet(test_file, args.test_aug, args.img_size, args.dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "val(args, model, test_loader, test_file) # this should match value in spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# Helpers\n",
    "\n",
    "# Font Size\n",
    "import matplotlib\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 30}\n",
    "\n",
    "import torch \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unnormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def read_resize_image(img, new_width=320, new_height=320):\n",
    "    img = ImageOps.fit(img, (new_width, new_height), Image.BICUBIC)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "def to_img_(ten):\n",
    "    curr_img = ten.detach().to(torch.device('cpu'))\n",
    "    curr_img = unnormalize(curr_img,\n",
    "                           torch.tensor([0, 0, 0]), # mean and std\n",
    "                           torch.tensor([1, 1, 1])) \n",
    "    curr_img = curr_img.permute((1, 2, 0))\n",
    "    curr_img = (curr_img.numpy()*255).astype(np.uint8)\n",
    "    curr_img = Image.fromarray(np.uint8(curr_img)).convert('RGB')\n",
    "    curr_img = read_resize_image(curr_img)\n",
    "    return curr_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"./checkpoint/analysis/prediction_visualizations/\" + exp_name\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "\n",
    "for index, data in enumerate(tqdm(test_loader)):\n",
    "\n",
    "    # Load data\n",
    "    img = data[\"img\"].cuda()\n",
    "    img_path = data[\"img_path\"]\n",
    "    target = data[\"target\"].cuda()\n",
    "\n",
    "    # Get class names from labels\n",
    "    post = torch.where(target[0]  > 0.5)[0].cpu().numpy()\n",
    "    gts = []\n",
    "    for k in post:\n",
    "        gts.append(class_dict[args.dataset][k])\n",
    "\n",
    "    if len(gts) == 3:\n",
    "        # Predict\n",
    "        logit = model(img).squeeze(0)\n",
    "        logit = nn.Sigmoid()(logit)\n",
    "\n",
    "        # Get class names from predictions\n",
    "        pos = torch.where(logit > 0.5)[0].cpu().numpy()\n",
    "        predictions = []\n",
    "        for k in pos:\n",
    "            predictions.append(class_dict[args.dataset][k])\n",
    "\n",
    "        # Do this for adding preds as captions with images\n",
    "        # #print(predictions, gts)\n",
    "        # caption = str(predictions)[1:-1]\n",
    "        # caption_gt = str(gts)[1:-1]\n",
    "        # fig = plt.figure()\n",
    "        # im = to_img_(img[0])\n",
    "        # plt.imshow(im)\n",
    "        # plt.axis(\"off\")\n",
    "        # fig.text(.5, .07, caption, ha='center', size=11)\n",
    "        # fig.text(.5, .008, \"GT: \" + caption_gt, ha='center', size=9)\n",
    "        # plt.savefig(LOG_PATH + \"/\" + f\"{index}.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=300)\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "        # Do this for adding preds as captions inside of the images\n",
    "        im = to_img_(img[0])\n",
    "        draw = im\n",
    "\n",
    "        gt = gts \n",
    "        yhat = predictions\n",
    "\n",
    "        if gt[0] in yhat:\n",
    "            image = cv2.putText(draw, f\"{gt[0]}\", (30, 220), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            image = cv2.putText(draw, f\"{gt[0]}\", (30, 220), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if gt[1] in yhat:\n",
    "            image = cv2.putText(draw, f\"{gt[1]}\", (30, 260), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            image = cv2.putText(draw, f\"{gt[1]}\", (30, 260), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if gt[2] in yhat:\n",
    "            image = cv2.putText(draw, f\"{gt[2]}\", (30, 300), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            image = cv2.putText(draw, f\"{gt[2]}\", (30, 300), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        bgr=cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(LOG_PATH + \"/\" + f\"{index}.png\",bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('maskrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35d972689a4ebd6112cf5bf9eea2c3bb189b2972b77b117bc02bba8b4bbbd65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
